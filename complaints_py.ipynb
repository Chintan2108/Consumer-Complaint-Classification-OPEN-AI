{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "complaints.py",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chintan2108/Consumer-Complaint-Classification-OPEN-AI/blob/master/complaints_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWFIUiSz12LZ",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "3c17bed5-ec96-4e92-c30b-3bca38b3f74e"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-323462dc-3ef0-40e7-b57f-d397f2a2186e\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-323462dc-3ef0-40e7-b57f-d397f2a2186e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"chintanmaniyar\",\"key\":\"cab4d845ce4566a2764d8aeb4c9cce41\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLzeFMtA14JD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCh-frUd3XtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYjwadJG3d5l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "cd04037c-a268-422a-cdbf-54728b5468b1"
      },
      "source": [
        "!kaggle datasets list"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "ref                                                       title                                              size  lastUpdated          downloadCount  \n",
            "--------------------------------------------------------  ------------------------------------------------  -----  -------------------  -------------  \n",
            "therohk/ireland-historical-news                           The Irish Times - Waxy-Wany News                   47MB  2019-08-24 15:36:54            571  \n",
            "dgomonov/new-york-city-airbnb-open-data                   New York City Airbnb Open Data                      2MB  2019-08-12 16:24:45           6619  \n",
            "lakshyaag/india-trade-data                                India - Trade Data                                  1MB  2019-08-16 16:13:58           4137  \n",
            "AnalyzeBoston/crimes-in-boston                            Crimes in Boston                                   10MB  2018-09-04 17:56:03          15987  \n",
            "jolasa/waves-measuring-buoys-data-mooloolaba              Waves Measuring Buoys Data                        599KB  2019-07-07 16:59:44           2000  \n",
            "citizen-ds-ghana/health-facilities-gh                     Ghana Health Facilities                            84KB  2018-09-03 01:19:24           1555  \n",
            "doit-intl/autotel-shared-car-locations                    Shared Cars Locations                              78MB  2019-01-10 13:06:00           1843  \n",
            "ma7555/schengen-visa-stats                                Schengen Visa Stats 2017/2018                       1MB  2019-07-25 10:55:37            615  \n",
            "dareenalharthi/jamalon-arabic-books-dataset               Jamalon Arabic Books Dataset                        1MB  2019-08-15 18:58:06            162  \n",
            "samhiatt/xenocanto-avian-vocalizations-canv-usa           Avian Vocalizations from CA & NV, USA               1GB  2019-08-10 00:16:10            115  \n",
            "Madgrades/uw-madison-courses                              UW Madison Courses and Grades 2006-2017            90MB  2018-05-15 18:58:25           1586  \n",
            "r3w0p4/bournemouth-venues                                 Venues in Bournemouth                               3KB  2019-07-08 14:26:48            985  \n",
            "lishuyangkaggle/cocktails-hotaling-co                     Cocktails (Hotaling & Co.)                         75KB  2019-07-08 23:49:34            545  \n",
            "bradklassen/pga-tour-20102018-data                        PGA Tour Golf Data                                 94MB  2019-08-30 00:09:59           4737  \n",
            "codersree/mount-rainier-weather-and-climbing-data         Mount Rainier Weather and Climbing Data            25KB  2019-08-27 23:33:36            538  \n",
            "datafiniti/pizza-restaurants-and-the-pizza-they-sell      Pizza Restaurants and the Pizza They Sell         850KB  2019-05-30 00:21:21           7339  \n",
            "inIT-OWL/vega-shrinkwrapper-runtofailure-data             Vega shrink-wrapper component degradation         408KB  2018-11-12 13:03:43            330  \n",
            "citylines/city-lines                                      Transit systems of world                            3MB  2019-03-25 16:53:05           2699  \n",
            "university-of-edinburgh/peace-agreements-dataset          Peace Agreements Dataset                            1MB  2018-03-05 16:39:50            946  \n",
            "martj42/international-football-results-from-1872-to-2017  International football results from 1872 to 2019  518KB  2019-07-22 17:03:13          17516  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HaaSovl3itt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "d872f2e2-1ba6-4fc7-9fbc-b16aa1314705"
      },
      "source": [
        "# coding: utf-8\n",
        "\n",
        "# # Multiclass Classification For User Complaints in Automotive\n",
        "# \n",
        "# ## Introduction\n",
        "# This is an NLP-based problem solving approach for the dataset available at http://www.cs.toronto.edu/~complingweb/data/karaOne/karaOne.html\n",
        "#domain - automotive \n",
        "import nltk\n",
        "import pickle\n",
        "import gensim\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import wordnet as wn\n",
        "from stop_words import get_stop_words\n",
        "import re, sys, math, string\n",
        "import calendar as cal\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "import logging\n",
        "from gensim.models import word2vec\n",
        "\n",
        "#from textblob import TextBlob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from  sklearn.calibration import CalibratedClassifierCV\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
        "from keras.layers.core import Reshape, Flatten\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "import altair as alt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from numpy import array\n",
        "\n",
        "main_df = pd.read_csv('data/Consumer_Complaints.csv')\n",
        "\n",
        "\n",
        "stplist = ['title', 'body', 'xxxx']\n",
        "english_stopwords = get_stop_words(language='english')\n",
        "english_stopwords += stplist\n",
        "english_stopwords = list(set(english_stopwords))\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"\n",
        "    Function that determines the the Part-of-speech (POS) tag.\n",
        "    Acts as input to lemmatizer. Result is of the form: [('complaint', 'NN'), ... ]\n",
        "    \"\"\"\n",
        "    if word.startswith('N'):\n",
        "        return wn.NOUN\n",
        "    elif word.startswith('V'):\n",
        "        return wn.VERB\n",
        "    elif word.startswith('J'):\n",
        "        return wn.ADJ\n",
        "    elif word.startswith('R'):\n",
        "        return wn.ADV\n",
        "    else:\n",
        "        return wn.NOUN\n",
        "\n",
        "\n",
        "def clean_up(text):\n",
        "    \"\"\"\n",
        "    Function to clean data.\n",
        "    Steps:\n",
        "    - Removing special characters, numbers\n",
        "    - Lemmatization\n",
        "    - Stop-words removal\n",
        "    - Getting a unique list of words\n",
        "    - TODO: try removing names and company names like Navient (Proper nouns)\n",
        "    \"\"\"\n",
        "    #lemma = WordNetLemmatizer()\n",
        "    lemmatizer = nltk.WordNetLemmatizer().lemmatize\n",
        "    text = re.sub('\\W+', ' ', str(text))\n",
        "    text = re.sub(r'[0-9]+', '', text.lower())\n",
        "    # correcting spellings of words using TextBlob - user complaints are bound to have spelling mistakes\n",
        "    # However, this idea was later dropped because TextBlob may change the words.\n",
        "    # text = TextBlob(text).correct()\n",
        "    word_pos = nltk.pos_tag(nltk.word_tokenize(text))\n",
        "    normalized_text_lst = [lemmatizer(x[0], get_wordnet_pos(x[1])).lower() for x in word_pos]\n",
        "    stop_words_free = [i for i in normalized_text_lst if i not in english_stopwords and len(i) > 3]\n",
        "    stop_words_free = list(set(stop_words_free))\n",
        "    return(stop_words_free)\n",
        "\n",
        "\n",
        "def get_average_word2vec(complaints_lst, model, num_features=300):\n",
        "    \"\"\"\n",
        "    Function to average the vectors in a list.\n",
        "    Say a list contains 'flower' and 'leaf'. Then this function gives - model[flower] + model[leaf]/2\n",
        "    - index2words gets the list of words in the model.\n",
        "    - Gets the list of words that are contained in index2words (vectorized_lst) and \n",
        "      the number of those words (nwords).\n",
        "    - Gets the average using these two and numpy.\n",
        "    \"\"\"\n",
        "    #complaint_feature_vecs = np.zeros((len(complaints_lst),num_features), dtype=\"float32\") #?used?\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    vectorized_lst = []\n",
        "    vectorized_lst = [model[word] if word in index2word_set else np.zeros(num_features) for word in complaints_lst]    \n",
        "    nwords = len(vectorized_lst)\n",
        "    summed = np.sum(vectorized_lst, axis=0)\n",
        "    averaged_vector = np.divide(summed, nwords)\n",
        "    return averaged_vector\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# ## Technique 2: Word2Vec\n",
        "# I tried creating my own model for Word2Vec. However, this only contained 17million words, as opposed to Google's GoogleNews' pretrained Word2Vec model. So, I chose to go ahead with the pre-trained model.\n",
        "# In lieu of time, I couldn't do this - but I would have preferred to complement the Google Word2Vec model with words from this dataset. This Word2Vec model is up until 2013, post which slang/other important words might have been introduced in the vocabulary. \n",
        "# Of course, these words could also be company-complaint specific. For example, for ATB Bank, someone might be using ATB bank or a specific Policy name like ATBUltraInsurance. These would also be removed.\n",
        "# Apart from this, these complaints contain a lot of spelling mistakes and words joined together. Such as: `immeditalely`, `demaging`,  `practiciing`, etc. (shown as missing_words in the cells below), and two words joined together into one word, such as 'givenrequesting'.\n",
        "# I tried looking into it and found out about a library called TextBlob. However, people also warned against its used because it might not always be right. So I chose to not use it and skip over these words for now.\n",
        "# There were also short forms not detected by the model.\n",
        "\n",
        "\n",
        "# Creating a Word2Vec model using training set\n",
        "vocabulary_of_all_words = input_df['complaint'].tolist()\n",
        "num_features = 300\n",
        "min_word_count = 10                      \n",
        "num_workers = 8\n",
        "context = 10          # Context window size                                                                                    \n",
        "downsampling = 1e-3   # Downsampling for frequent words\n",
        "word2vec_model_name = \"trained_models/300features_10minwords_10context1\"\n",
        "word2vec_complaints = word2vec.Word2Vec(vocabulary_of_all_words, workers=num_workers, size=num_features, \n",
        "                                   min_count=min_word_count, window=context, sample=downsampling)\n",
        "word2vec_complaints.save(word2vec_model_name)\n",
        "\n",
        "# Fetching trained model to save time.\n",
        "word2vec_complaints = gensim.models.Word2Vec.load(word2vec_model_name)\n",
        "\n",
        "vocab_lst_flat = [item for sublist in vocabulary_of_all_words for item in sublist]\n",
        "vocab_lst_flat = list(set(vocab_lst_flat))\n",
        "# Loading a pre-trained GoogleNews model\n",
        "# word2vec_model = KeyedVectors.load_word2vec_format(\"trained_models/GoogleNews-vectors-negative300.bin\", binary=True)\n",
        "\n",
        "# Exploring this model to see how well it has trained and checking for spelling mistakes in user-complaints\n",
        "try:\n",
        "    word2vec_complaints.wv.most_similar(\"good\")\n",
        "except KeyError:\n",
        "    print(\"Sorry, this word doesn't exist in the vocabulary.\")\n",
        "    \n",
        "words_not_present = 0\n",
        "words_present = 0\n",
        "total_unique_tokens = len(set(vocab_lst_flat))\n",
        "missing_words = []\n",
        "for i in vocab_lst_flat:\n",
        "    try:\n",
        "        p = word2vec_complaints[i]\n",
        "        words_present+=1\n",
        "    except KeyError:\n",
        "        missing_words.append(i)\n",
        "        words_not_present+=1\n",
        "print(words_present, words_not_present, total_unique_tokens)\n",
        "\n",
        "# Examples of spelling mistakes, grammatical errors, etc.\n",
        "print(missing_words[:20])\n",
        "\n",
        "\n",
        "# #### Choosing a Word2Vec Model\n",
        "# - The Google word2vec model isn't able to account for a lot of words. It can be made better by retraining on more words from the training set. However, a lot of these words are spelling mistakes.\n",
        "# - The presence of 'xxxx', 'xx', etc. in various forms is a simple fix which can also be implemented.\n",
        "# - Initially, I had planned to use Google's pretrained Word2Vec model. However, after waiting for hours for training on Google word2vec model, I switched back to the Word2Vec model for want of speed.\n",
        "\n",
        "\n",
        "# # These take a very long time to be averaged. Commenting this code and reading from file the saved output.\n",
        "# embeddings_df = input_df['complaint'].apply(lambda complaint: get_average_word2vec(complaint, word2vec_complaints, \n",
        "#                                                                                    num_features)).to_frame()\n",
        "# col_lst = []\n",
        "# for i in range(num_features):\n",
        "#     col_lst.append('vec_'+str(i+1))\n",
        "# # Easy to write to file and process when exploded into columns\n",
        "# exploded_em_df = pd.DataFrame(embeddings_df.complaint.tolist(), columns=col_lst)\n",
        "# exploded_em_df = pd.DataFrame(embeddings_df)['complaint'].apply(pd.Series)\n",
        "# exploded_em_df.head()\n",
        "# exploded_em_df.to_csv(\"data/modified/vocab_trained_word2Vec.csv\", index=False)\n",
        "\n",
        "exploded_em_df = pd.read_csv('data/modified/vocab_trained_word2Vec.csv')\n",
        "print(\"Word2Vec output:\\n\")\n",
        "exploded_em_df.head()\n",
        "\n",
        "input_df = input_df.reset_index(drop=True)\n",
        "vectorized_df = pd.concat([exploded_em_df, input_df[['product']]], axis=1)                        \n",
        "vectorized_df = shuffle(vectorized_df)\n",
        "\n",
        "if vectorized_df[vectorized_df.isnull().any(axis=1)].empty:\n",
        "    res = \"True\" # No NaNs exist in the cleaned dataset.\n",
        "else:\n",
        "    res = \"False\"\n",
        "print(res)\n",
        "print(vectorized_df.shape)\n",
        "if not res:\n",
        "    vectorized_df[vectorized_df.isnull().any(axis=1)]\n",
        "    vectorized_df.dropna(axis=0, how='any')\n",
        "    print(vectorized_df.shape)\n",
        "\n",
        "\n",
        "# ### Training and Test Sets]\n",
        "\n",
        "vectorized_data = np.array(vectorized_df.drop('product', axis=1))\n",
        "vectorized_target = np.array(vectorized_df['product'])\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(vectorized_data, vectorized_target,\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=123)\n",
        "\n",
        "\n",
        "\n",
        "# 3. Deep Neural Network - CNN: Upon reading online some discussion on this, I thought of implementing CNNs. It said - what has recently been shown to work much better and simpler than RNNs is using word vectors, pre-trained on a large corpus, as features to the neural network. RNNs were called 'slow and fickle to train'.\n",
        "\n",
        "\n",
        "# Model 3: CNN using Keras\n",
        "from keras.layers import Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "NUM_WORDS = 20000\n",
        "texts = train_df.complaints_untokenized\n",
        "products_unique = vectorized_df['product'].unique()\n",
        "\n",
        "dict_products = {}\n",
        "for i, complaint in enumerate(products_unique):\n",
        "    dict_products[complaint] = i\n",
        "labels = vectorized_df['product'].apply(lambda x:dict_products[x])\n",
        "\n",
        "vocab_lst_flat = [item for sublist in vocabulary_of_all_words for item in sublist]\n",
        "\n",
        "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
        "                      lower=True)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences_train = tokenizer.texts_to_sequences(texts)\n",
        "sequences_valid=tokenizer.texts_to_sequences(val_df.complaints_untokenized)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "EMBEDDING_DIM=300\n",
        "vocabulary_size=min(len(word_index) + 1, NUM_WORDS)\n",
        "embedding_layer = Embedding(vocabulary_size, EMBEDDING_DIM)\n",
        "\n",
        "    \n",
        "train_df = train_df.drop(val_df.index)\n",
        "                    \n",
        "size_train = len(train_x)\n",
        "size_test = len(test_x)\n",
        "output_labels_unique = np.asarray(sorted(list(set(labels))))\n",
        "\n",
        "X_train = pad_sequences(sequences_train)\n",
        "X_val = pad_sequences(sequences_valid,maxlen=X_train.shape[1]) #test\n",
        "# convert into dummy representation of the output labels\n",
        "y_train = to_categorical(np.asarray(labels[train_df.index]))\n",
        "y_val = to_categorical(np.asarray(labels[val_df.index]))\n",
        "\n",
        "sequence_length = X_train.shape[1]\n",
        "filter_sizes = [3,4,5]\n",
        "num_filters = 100\n",
        "drop = 0.5\n",
        "\n",
        "output_dim = len(products_unique)\n",
        "\n",
        "print('Shape of X train and X test tensors:', X_train.shape, X_val.shape)\n",
        "print('Shape of label train and test tensors:', y_train.shape, y_val.shape)\n",
        "\n",
        "inputs = Input(shape=(sequence_length,))\n",
        "embedding = embedding_layer(inputs)\n",
        "reshape = Reshape((sequence_length, EMBEDDING_DIM, 1))(embedding)\n",
        "\n",
        "conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM), activation='relu', \n",
        "                                kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM), activation='relu', \n",
        "                                kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "conv_2 = Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM), activation='relu', \n",
        "                                kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "\n",
        "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
        "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
        "maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n",
        "\n",
        "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
        "flatten = Flatten()(merged_tensor)\n",
        "reshape = Reshape((3*num_filters,))(flatten)\n",
        "dropout = Dropout(drop)(flatten)\n",
        "output = Dense(units=output_dim, activation='softmax', kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
        "\n",
        "cnn_model = Model(inputs, output)\n",
        "adam = Adam(lr=1e-3)\n",
        "cnn_model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=adam,\n",
        "              metrics=['acc'])\n",
        "callbacks = [EarlyStopping(monitor='val_loss')]\n",
        "\n",
        "cnn_model.fit(X_train, y_train, batch_size=1000, epochs=10, verbose=1, validation_data=(X_val, y_val),\n",
        "                callbacks=callbacks)\n",
        "\n",
        "# Predicting on the test set\n",
        "sequences_test = test_x\n",
        "X_test = pad_sequences(sequences_test, maxlen=X_train.shape[1])\n",
        "cnn_preds = cnn_model.predict(X_test)\n",
        "print(\"Predictions from CNN completed.\")\n",
        "\n",
        "\n",
        "cnn_results = pd.DataFrame(data={\"actual_label\":test_y, \"predicted_label\":cnn_preds})\n",
        "# Accuracy: wherever the labels were correctly predicted.\n",
        "cnn_results['correctly_predicted'] = np.where(cnn_results['actual_label'] == cnn_results['predicted_label'], \n",
        "                                                1, 0)\n",
        "cnn_accuracy = (naive_results['correctly_predicted'].sum()/cnn_results.shape[0])*100\n",
        "print(\"Accuracy of the CNN Model is: {0:.2f}.\".format(cnn_accuracy))\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# ## Conclusion\n",
        "# \n",
        "# - The model that performed best was: CNN with SQUAD liek pre-training. It gave an accuracy measure of: 75.30%. This was obtained with the word2Vec model made out of the the training set. Further, \n",
        "# the gensim word model was used to create the sentence level representations of the consumer complaints post the pre-training\n",
        "# -----------------------------------------------------------------------------------------------------------------"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-1e7363bd5980>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwordnet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcalendar\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stop_words'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RepLlhG3q1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}