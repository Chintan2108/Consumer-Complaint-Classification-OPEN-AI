{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "complaints.py",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chintan2108/Consumer-Complaint-Classification-OPEN-AI/blob/master/complaints_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWkT2VzPTvSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "\n",
        "# # Multiclass Classification For User Complaints in Banking\n",
        "# \n",
        "# ## Introduction\n",
        "# This is an NLP-based problem solving approach for the dataset available at http://www.cs.toronto.edu/~complingweb/data/karaOne/karaOne.html\n",
        "#domain - automotive \n",
        "\n",
        "import nltk\n",
        "import pickle\n",
        "import gensim\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import wordnet as wn\n",
        "from stop_words import get_stop_words\n",
        "import re, sys, math, string\n",
        "import calendar as cal\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "import logging\n",
        "from gensim.models import word2vec\n",
        "\n",
        "#from textblob import TextBlob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from  sklearn.calibration import CalibratedClassifierCV\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
        "from keras.layers.core import Reshape, Flatten\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "import altair as alt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from numpy import array\n",
        "\n",
        "main_df = pd.read_csv('data/Consumer_Complaints.csv')\n",
        "\n",
        "\n",
        "stplist = ['title', 'body', 'xxxx']\n",
        "english_stopwords = get_stop_words(language='english')\n",
        "english_stopwords += stplist\n",
        "english_stopwords = list(set(english_stopwords))\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"\n",
        "    Function that determines the the Part-of-speech (POS) tag.\n",
        "    Acts as input to lemmatizer. Result is of the form: [('complaint', 'NN'), ... ]\n",
        "    \"\"\"\n",
        "    if word.startswith('N'):\n",
        "        return wn.NOUN\n",
        "    elif word.startswith('V'):\n",
        "        return wn.VERB\n",
        "    elif word.startswith('J'):\n",
        "        return wn.ADJ\n",
        "    elif word.startswith('R'):\n",
        "        return wn.ADV\n",
        "    else:\n",
        "        return wn.NOUN\n",
        "\n",
        "\n",
        "def clean_up(text):\n",
        "    \"\"\"\n",
        "    Function to clean data.\n",
        "    Steps:\n",
        "    - Removing special characters, numbers\n",
        "    - Lemmatization\n",
        "    - Stop-words removal\n",
        "    - Getting a unique list of words\n",
        "    - TODO: try removing names and company names like Navient (Proper nouns)\n",
        "    \"\"\"\n",
        "    #lemma = WordNetLemmatizer()\n",
        "    lemmatizer = nltk.WordNetLemmatizer().lemmatize\n",
        "    text = re.sub('\\W+', ' ', str(text))\n",
        "    text = re.sub(r'[0-9]+', '', text.lower())\n",
        "    # correcting spellings of words using TextBlob - user complaints are bound to have spelling mistakes\n",
        "    # However, this idea was later dropped because TextBlob may change the words.\n",
        "    # text = TextBlob(text).correct()\n",
        "    word_pos = nltk.pos_tag(nltk.word_tokenize(text))\n",
        "    normalized_text_lst = [lemmatizer(x[0], get_wordnet_pos(x[1])).lower() for x in word_pos]\n",
        "    stop_words_free = [i for i in normalized_text_lst if i not in english_stopwords and len(i) > 3]\n",
        "    stop_words_free = list(set(stop_words_free))\n",
        "    return(stop_words_free)\n",
        "\n",
        "\n",
        "def get_average_word2vec(complaints_lst, model, num_features=300):\n",
        "    \"\"\"\n",
        "    Function to average the vectors in a list.\n",
        "    Say a list contains 'flower' and 'leaf'. Then this function gives - model[flower] + model[leaf]/2\n",
        "    - index2words gets the list of words in the model.\n",
        "    - Gets the list of words that are contained in index2words (vectorized_lst) and \n",
        "      the number of those words (nwords).\n",
        "    - Gets the average using these two and numpy.\n",
        "    \"\"\"\n",
        "    #complaint_feature_vecs = np.zeros((len(complaints_lst),num_features), dtype=\"float32\") #?used?\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    vectorized_lst = []\n",
        "    vectorized_lst = [model[word] if word in index2word_set else np.zeros(num_features) for word in complaints_lst]    \n",
        "    nwords = len(vectorized_lst)\n",
        "    summed = np.sum(vectorized_lst, axis=0)\n",
        "    averaged_vector = np.divide(summed, nwords)\n",
        "    return averaged_vector\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# ## Technique 2: Word2Vec\n",
        "# I tried creating my own model for Word2Vec. However, this only contained 17million words, as opposed to Google's GoogleNews' pretrained Word2Vec model (negative 300 bin layers). So, I chose to go ahead with the pre-trained model.\n",
        "# In lieu of time, I couldn't do this - but I would have preferred to complement the Google Word2Vec model with words from this dataset. This Word2Vec model is up until 2013, post which slang/other important words might have been introduced in the vocabulary. \n",
        "# Of course, these words could also be company-complaint specific. For example, for ATB Bank, someone might be using ATB bank or a specific Policy name like ATBUltraInsurance. These would also be removed.\n",
        "# Apart from this, these complaints contain a lot of spelling mistakes and words joined together. Such as: `immeditalely`, `demaging`,  `practiciing`, etc. (shown as missing_words in the cells below), and two words joined together into one word, such as 'givenrequesting'.\n",
        "# I tried looking into it and found out about a library called TextBlob. However, people also warned against its used because it might not always be right. So I chose to not use it and skip over these words for now.\n",
        "# There were also short forms not detected by the model.\n",
        "\n",
        "\n",
        "# Creating a Word2Vec model using training set\n",
        "vocabulary_of_all_words = input_df['complaint'].tolist()\n",
        "num_features = 300\n",
        "min_word_count = 10                      \n",
        "num_workers = 8\n",
        "context = 10          # Context window size                                                                                    \n",
        "downsampling = 1e-3   # Downsampling for frequent words\n",
        "word2vec_model_name = \"trained_models/300features_10minwords_10context1\"\n",
        "word2vec_complaints = word2vec.Word2Vec(vocabulary_of_all_words, workers=num_workers, size=num_features, \n",
        "                                   min_count=min_word_count, window=context, sample=downsampling)\n",
        "word2vec_complaints.save(word2vec_model_name)\n",
        "\n",
        "# Fetching trained model to save time.\n",
        "word2vec_complaints = gensim.models.Word2Vec.load(word2vec_model_name)\n",
        "\n",
        "vocab_lst_flat = [item for sublist in vocabulary_of_all_words for item in sublist]\n",
        "vocab_lst_flat = list(set(vocab_lst_flat))\n",
        "# Loading a pre-trained GoogleNews model\n",
        "# word2vec_model = KeyedVectors.load_word2vec_format(\"trained_models/GoogleNews-vectors-negative300.bin\", binary=True)\n",
        "\n",
        "# Exploring this model to see how well it has trained and checking for spelling mistakes in user-complaints\n",
        "try:\n",
        "    word2vec_complaints.wv.most_similar(\"good\")\n",
        "except KeyError:\n",
        "    print(\"Sorry, this word doesn't exist in the vocabulary.\")\n",
        "    \n",
        "words_not_present = 0\n",
        "words_present = 0\n",
        "total_unique_tokens = len(set(vocab_lst_flat))\n",
        "missing_words = []\n",
        "for i in vocab_lst_flat:\n",
        "    try:\n",
        "        p = word2vec_complaints[i]\n",
        "        words_present+=1\n",
        "    except KeyError:\n",
        "        missing_words.append(i)\n",
        "        words_not_present+=1\n",
        "print(words_present, words_not_present, total_unique_tokens)\n",
        "\n",
        "# Examples of spelling mistakes, grammatical errors, etc.\n",
        "print(missing_words[:20])\n",
        "\n",
        "\n",
        "# #### Choosing a Word2Vec Model\n",
        "# - The Google word2vec model isn't able to account for a lot of words. It can be made better by retraining on more words from the training set. However, a lot of these words are spelling mistakes.\n",
        "# - The presence of 'xxxx', 'xx', etc. in various forms is a simple fix which can also be implemented.\n",
        "# - Initially, I had planned to use Google's pretrained Word2Vec model. However, after waiting for hours for training on Google word2vec model, I switched back to the Word2Vec model for want of speed.\n",
        "\n",
        "\n",
        "# # These take a very long time to be averaged. Commenting this code and reading from file the saved output.\n",
        "# embeddings_df = input_df['complaint'].apply(lambda complaint: get_average_word2vec(complaint, word2vec_complaints, \n",
        "#                                                                                    num_features)).to_frame()\n",
        "# col_lst = []\n",
        "# for i in range(num_features):\n",
        "#     col_lst.append('vec_'+str(i+1))\n",
        "# # Easy to write to file and process when exploded into columns\n",
        "# exploded_em_df = pd.DataFrame(embeddings_df.complaint.tolist(), columns=col_lst)\n",
        "# exploded_em_df = pd.DataFrame(embeddings_df)['complaint'].apply(pd.Series)\n",
        "# exploded_em_df.head()\n",
        "# exploded_em_df.to_csv(\"data/modified/vocab_trained_word2Vec.csv\", index=False)\n",
        "\n",
        "exploded_em_df = pd.read_csv('data/modified/vocab_trained_word2Vec.csv')\n",
        "print(\"Word2Vec output:\\n\")\n",
        "exploded_em_df.head()\n",
        "\n",
        "input_df = input_df.reset_index(drop=True)\n",
        "vectorized_df = pd.concat([exploded_em_df, input_df[['product']]], axis=1)                        \n",
        "vectorized_df = shuffle(vectorized_df)\n",
        "\n",
        "if vectorized_df[vectorized_df.isnull().any(axis=1)].empty:\n",
        "    res = \"True\" # No NaNs exist in the cleaned dataset.\n",
        "else:\n",
        "    res = \"False\"\n",
        "print(res)\n",
        "print(vectorized_df.shape)\n",
        "if not res:\n",
        "    vectorized_df[vectorized_df.isnull().any(axis=1)]\n",
        "    vectorized_df.dropna(axis=0, how='any')\n",
        "    print(vectorized_df.shape)\n",
        "\n",
        "\n",
        "# ### Training and Test Sets]\n",
        "\n",
        "vectorized_data = np.array(vectorized_df.drop('product', axis=1))\n",
        "vectorized_target = np.array(vectorized_df['product'])\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(vectorized_data, vectorized_target,\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=123)\n",
        "\n",
        "\n",
        "\n",
        "# 3. Deep Neural Network - CNN: Upon reading online some discussion on this, I thought of implementing CNNs. It said - what has recently been shown to work much better and simpler than RNNs is using word vectors, pre-trained on a large corpus, as features to the neural network. RNNs were called 'slow and fickle to train'.\n",
        "\n",
        "\n",
        "# Model 3: CNN using Keras\n",
        "from keras.layers import Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "NUM_WORDS = 20000\n",
        "texts = train_df.complaints_untokenized\n",
        "products_unique = vectorized_df['product'].unique()\n",
        "\n",
        "dict_products = {}\n",
        "for i, complaint in enumerate(products_unique):\n",
        "    dict_products[complaint] = i\n",
        "labels = vectorized_df['product'].apply(lambda x:dict_products[x])\n",
        "\n",
        "vocab_lst_flat = [item for sublist in vocabulary_of_all_words for item in sublist]\n",
        "\n",
        "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
        "                      lower=True)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences_train = tokenizer.texts_to_sequences(texts)\n",
        "sequences_valid=tokenizer.texts_to_sequences(val_df.complaints_untokenized)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "EMBEDDING_DIM=300\n",
        "vocabulary_size=min(len(word_index) + 1, NUM_WORDS)\n",
        "embedding_layer = Embedding(vocabulary_size, EMBEDDING_DIM)\n",
        "\n",
        "    \n",
        "train_df = train_df.drop(val_df.index)\n",
        "                    \n",
        "size_train = len(train_x)\n",
        "size_test = len(test_x)\n",
        "output_labels_unique = np.asarray(sorted(list(set(labels))))\n",
        "\n",
        "X_train = pad_sequences(sequences_train)\n",
        "X_val = pad_sequences(sequences_valid,maxlen=X_train.shape[1]) #test\n",
        "# convert into dummy representation of the output labels\n",
        "y_train = to_categorical(np.asarray(labels[train_df.index]))\n",
        "y_val = to_categorical(np.asarray(labels[val_df.index]))\n",
        "\n",
        "sequence_length = X_train.shape[1]\n",
        "filter_sizes = [3,4,5]\n",
        "num_filters = 100\n",
        "drop = 0.5\n",
        "\n",
        "output_dim = len(products_unique)\n",
        "\n",
        "print('Shape of X train and X test tensors:', X_train.shape, X_val.shape)\n",
        "print('Shape of label train and test tensors:', y_train.shape, y_val.shape)\n",
        "\n",
        "inputs = Input(shape=(sequence_length,))\n",
        "embedding = embedding_layer(inputs)\n",
        "reshape = Reshape((sequence_length, EMBEDDING_DIM, 1))(embedding)\n",
        "\n",
        "conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM), activation='relu', \n",
        "                                kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM), activation='relu', \n",
        "                                kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "conv_2 = Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM), activation='relu', \n",
        "                                kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "\n",
        "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
        "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
        "maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n",
        "\n",
        "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
        "flatten = Flatten()(merged_tensor)\n",
        "reshape = Reshape((3*num_filters,))(flatten)\n",
        "dropout = Dropout(drop)(flatten)\n",
        "output = Dense(units=output_dim, activation='softmax', kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
        "\n",
        "cnn_model = Model(inputs, output)\n",
        "adam = Adam(lr=1e-3)\n",
        "cnn_model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=adam,\n",
        "              metrics=['acc'])\n",
        "callbacks = [EarlyStopping(monitor='val_loss')]\n",
        "\n",
        "cnn_model.fit(X_train, y_train, batch_size=1000, epochs=10, verbose=1, validation_data=(X_val, y_val),\n",
        "                callbacks=callbacks)\n",
        "\n",
        "# Predicting on the test set\n",
        "sequences_test = test_x\n",
        "X_test = pad_sequences(sequences_test, maxlen=X_train.shape[1])\n",
        "cnn_preds = cnn_model.predict(X_test)\n",
        "print(\"Predictions from CNN completed.\")\n",
        "\n",
        "\n",
        "cnn_results = pd.DataFrame(data={\"actual_label\":test_y, \"predicted_label\":cnn_preds})\n",
        "# Accuracy: wherever the labels were correctly predicted.\n",
        "cnn_results['correctly_predicted'] = np.where(cnn_results['actual_label'] == cnn_results['predicted_label'], \n",
        "                                                1, 0)\n",
        "cnn_accuracy = (naive_results['correctly_predicted'].sum()/cnn_results.shape[0])*100\n",
        "print(\"Accuracy of the CNN Model is: {0:.2f}.\".format(cnn_accuracy))\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# ## Conclusion\n",
        "# \n",
        "# - The model that performed best was: CNN with SQUAD liek pre-training. It gave an accuracy measure of: 75.30%. This was obtained with the word2Vec model made out of the the training set. Further, \n",
        "# the gensim word model was used to create the sentence level representations of the consumer complaints post the pre-training\n",
        "# -----------------------------------------------------------------------------------------------------------------"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
